#!/usr/bin/env python
# coding: utf-8

__author__ = "Yang Li"

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
import warnings
warnings.filterwarnings('ignore')

if __name__ == "__main__":
    # import the dataset.

    columns = ['Age', 'Work Class', 'Final Weight', 'Education', 'Education Number', 'Marital Status', 'Occupation',
              'Relationship', 'Race', 'Sex', 'Capital Gain', 'Capital Loss', 'Hours per Week', 'Country', 'Income']
    dataset = pd.read_csv('income-dataset.csv', names = columns)

    #convert the target column into numerical classes.
    labelEncoder = LabelEncoder()
    dataset['Income'] = labelEncoder.fit_transform(dataset['Income'])
    dataset['Age'] = pd.cut(dataset['Age'],
                                bins = [0, 20, 30, 35, 45, 50, 55, 100],
                                labels = ['0-20', '21-30', '31-35','36-45',
                                          '46-50', '51-54', '55-100'])
    dataset['Capital Diff'] = dataset['Capital Gain'] - dataset['Capital Loss']
    dataset.drop(['Capital Gain'], axis = 1, inplace = True)
    dataset.drop(['Capital Loss'], axis = 1, inplace = True)
    dataset['Capital Diff'] = pd.cut(dataset['Capital Diff'], bins = [-5000, 5000, 100000], labels = ['Minor', 'Major'])
    dataset.drop(['Final Weight'], axis = 1, inplace = True)
    dataset['Hours per Week'] = pd.cut(dataset['Hours per Week'],
                                       bins = [0, 30, 40, 100],
                                       labels = ['Lesser Hours', 'Normal Hours', 'Extra Hours'])
    dataset = dataset.drop(dataset[dataset['Work Class'] == ' ?'].index)
    dataset = dataset.drop(dataset[dataset['Work Class'] == ' Without-pay'].index)
    dataset = dataset.drop(dataset[dataset['Work Class'] == ' Never-worked'].index)
    education_classes = dataset['Education'].unique()
    for edu_class in education_classes:
        print("For {}, the Education Number is {}"
              .format(edu_class, dataset[dataset['Education'] == edu_class]['Education Number'].unique()))
    dataset['Race'].unique()
    dataset['Race'].replace([' Black', ' Asian-Pac-Islander', ' Amer-Indian-Eskimo', ' Other'],' Other', inplace = True)
    dataset = dataset.drop(dataset[dataset['Country'] == ' ?'].index)
    countries = np.array(dataset['Country'].unique())
    countries = np.delete(countries, 0)
    dataset['Country'].replace(countries, 'Other', inplace = True)
    # Education
    dataset.drop(['Education Number'], axis = 1, inplace = True)
    dataset['Education'].replace([' 11th', ' 9th', ' 7th-8th', ' 5th-6th', ' 10th', ' 1st-4th', ' Preschool', ' 12th'],
                                 ' School', inplace = True)
    dataset.to_csv('dataset_after_process_v2.csv')
    # ML
    y = dataset['Income']
    X = dataset.drop(['Income'], axis = 1)
    X = pd.get_dummies(X)
    X.to_csv('dataset_dummy_input_features_v2.csv')
    print("Total features: {}".format(X.shape[1]))
    # split the dataset into the training and testing data using `train_test_split`.
    from sklearn.model_selection import train_test_split
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 0)
    ## Applying Logistic Regression
    from sklearn.metrics import f1_score, accuracy_score
    from sklearn.linear_model import LogisticRegression
    classifier = LogisticRegression(random_state = 0)
    classifier.fit(X_train, y_train)
    y_pred = classifier.predict(X_test)
    print("{}:".format('Logistic Regression'))
    print("F1 score: {:.2f}".format(f1_score(y_test, y_pred)))
    accuracy = accuracy_score(y_test, y_pred) * 100
    print('accuracy is ' + str(accuracy))
    weight = classifier.coef_
    print(weight)
    from sklearn.naive_bayes import GaussianNB
    from sklearn.tree import DecisionTreeClassifier
    from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
    classifiers = [GaussianNB(),
                   LogisticRegression(random_state = 0),
                   DecisionTreeClassifier(random_state = 0),
                   RandomForestClassifier(n_estimators = 100, random_state = 0),
                   GradientBoostingClassifier(random_state = 0)]
    classifier_names = ["Gaussian Naive Bayes",
                        "Logistic Regression",
                        "Decision Tree Classifier",
                        "Random Forest Classifier",
                        "Gradient Boosting Classifier"]
    accuracies = []
    for i in range(len(classifiers)):
        classifier = classifiers[i]
        classifier.fit(X_train, y_train)
        y_pred = classifier.predict(X_test)
        print("{}:".format(classifier_names[i]))
        print("F1 score: {:.2f}".format(f1_score(y_test, y_pred)))
        accuracy = accuracy_score(y_test, y_pred)*100
        accuracies.append(accuracy)
    # Analysing Results
    from matplotlib.cm import rainbow
    plt.figure(figsize = (20, 12))
    colors = rainbow(np.linspace(0, 1, len(classifiers)))
    barplot = plt.bar(classifier_names, accuracies, color = colors)
    plt.yticks([0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100], fontsize = 16)
    plt.xticks(fontsize = 14)
    plt.xlabel("Classifiers", fontsize = 16)
    plt.ylabel("Accuracy", fontsize = 16)
    plt.title("Plot for accuracy of all classifiers", fontsize = 16)
    for i, bar in enumerate(barplot):
        plt.text(bar.get_x() + bar.get_width()/2 - 0.1,
                 bar.get_height()*1.02,
                 s = '{:.2f}%'.format(accuracies[i]),
                 fontsize = 16)
    plt.show()
